{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# import skimage\n",
    "# from skimage import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "# from pipeline import StableDiffusionPipeline\n",
    "from diffusers import UNet2DConditionModel, StableDiffusionImg2ImgPipeline, StableDiffusionPipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "SPLIT = 'light_and_dark_flex_to_dark'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"stabilityai/stable-diffusion-2-1-base\" \n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, revision=\"fp16\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "def load_embeddings(embed_path: str, \n",
    "                    model_path: str = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "                    ):\n",
    "\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        model_path, use_auth_token=True,\n",
    "        subfolder=\"tokenizer\")\n",
    "\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        model_path, use_auth_token=True,\n",
    "        subfolder=\"text_encoder\")\n",
    "    \n",
    "    print(len(tokenizer))\n",
    "\n",
    "    for token, token_embedding in torch.load(\n",
    "            embed_path, map_location=\"cpu\").items():\n",
    "\n",
    "        # add the token in tokenizer\n",
    "        num_added_tokens = tokenizer.add_tokens(token)\n",
    "\n",
    "        # resize the token embeddings\n",
    "        text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "        added_token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "        # get the old word embeddings\n",
    "        embeddings = text_encoder.get_input_embeddings()\n",
    "\n",
    "        # get the id for the token and assign new embeds\n",
    "        embeddings.weight.data[added_token_id] = \\\n",
    "            token_embedding.to(embeddings.weight.dtype)\n",
    "    print(len(tokenizer))\n",
    "\n",
    "    return tokenizer, text_encoder.to(device)\n",
    "\n",
    "### add textual inversion tokens\n",
    "embed_path = f\"../models/textual_inversion/{SPLIT}_seed={SEED}/aggregated_embeds_SEED={SEED}.pt\"\n",
    "print(embed_path)\n",
    "tokenizer, text_encoder = load_embeddings(\n",
    "                embed_path, model_path=MODEL_NAME)\n",
    "pipe.tokenizer = tokenizer\n",
    "pipe.text_encoder = text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train = f'../data_splits/train_lora_{SPLIT}_seed={SEED}.json'\n",
    "lora_path = f'../models/lora_weights/{SPLIT}_seed={SEED}'\n",
    "output_path = f'../inference_img/{SPLIT}_seed={SEED}_steps=100_strength=0.5_guidance=2'\n",
    "image_dir = '/data/derm_data/Fitzpatrick17k/finalfitz17k/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "pipe.load_lora_weights(lora_path)\n",
    "pipe.to(dtype=torch.float16)\n",
    "\n",
    "tone_mapper = {\n",
    "    1: 'a very light-skinned',\n",
    "    2: 'a light-skinned',\n",
    "    5: 'a dark-skinned',\n",
    "    6: 'a very dark-skinned',\n",
    "}\n",
    "\n",
    "token_mapper ={\n",
    "    \"basal cell carcinoma\": \"bas-class\",\n",
    "    \"folliculitis\": \"fol-class\",\n",
    "    \"nematode infection\": \"nem-class\",\n",
    "    \"neutrophilic dermatoses\": \"neu-class\",\n",
    "    \"prurigo nodularis\": \"pru-class\",\n",
    "    \"psoriasis\": \"pso-class\",\n",
    "    \"squamous cell carcinoma\": \"squ-class\",\n",
    "}\n",
    "\n",
    "with open(real_train, \"r\") as f:\n",
    "        real_train = json.load(f)\n",
    "        \n",
    "print(len(real_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for data in real_train[:10]:\n",
    "    dtype = data['label']\n",
    "    stype = data['skin_type']\n",
    "    img_id = data['image_path'].split('.')[0]\n",
    "    img = Image.open(f'{image_dir}/{data['image_path']}').convert(\"RGB\")\n",
    "    img = img.resize((512, 512), resample=PIL.Image.BILINEAR)\n",
    "    if stype not in [5, 6]:\n",
    "        stype = random.choice([5, 6])\n",
    "    images = pipe( \n",
    "        prompt=f\"An image of {token_mapper[dtype]} on the skin of {tone_mapper[stype]} individual\",\n",
    "        image=img,\n",
    "        strength=0.5, # default: 0.8,  0.7 - cannot change the skin color,  0.75 - change the skin color\n",
    "        num_inference_steps=100,\n",
    "        guidance_scale=2, # default: 7.5\n",
    "        num_images_per_prompt=5,\n",
    "    ).images\n",
    "    idx = 0\n",
    "    for image in images:\n",
    "        name = f\"{img_id}_{idx}\"\n",
    "        resized_img = image.resize(size=(256, 256))\n",
    "        resized_img.save(f'{output_path}/{name}.jpg')\n",
    "        res.append([name, dtype, stype])\n",
    "        idx += 1\n",
    "\n",
    "synthetic_train = pd.DataFrame(res, columns=['md5hash', 'label', 'fitzpatrick_scale'])\n",
    "synthetic_train.to_csv(f'{output_path}.csv', index=False) \n",
    "synthetic_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
